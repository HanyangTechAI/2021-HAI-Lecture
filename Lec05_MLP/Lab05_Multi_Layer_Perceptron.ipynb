{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Layer Perceptron #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 05 : Multi Layer Perceptron ##\n",
    "\n",
    "### Review ###  \n",
    "\n",
    "**모델(Model)**\n",
    "$y = h(W_2h(W_1x + b_1) + b_2)$  \n",
    "-> Model은 Weight Vector W와 bias vector b를 더한 뒤, Activation함수를 거치고, 이를 여려겹 쌓은 것.  \n",
    "\n",
    "**모수(Parameter)**  \n",
    "$W_1$, $W_2$, $b_1$, $b_2$ : Vectors of Weights and Bias\n",
    "\n",
    "**오차 함수(Error Function)**  \n",
    "Regression : MSE, $L = \\frac{1}{N}\\sum_{k=1}^N (y - \\hat{y})^2$  \n",
    "Classification : Cross Entropy : $L = -\\sum_i \\hat{y}_ilog(y_i)$  \n",
    "\n",
    "**학습 알고리즘(Learning Algorithm)**  \n",
    "Gradient Descent and Error Back Propagation  \n",
    "- 출력단의 Error를 앞쪽 Layer로 전파(Propagate)하며, Gradient가 감소하는 방향으로 Weight를 Update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset ###\n",
    "Multi Layer Perceptron은 기존에 Perceptron 알고리즘으로는 해결할 수 없었던 XOR 분류와 같은 문제를 해결할 수 있도록 해주었습니다.  \n",
    "마치, Logic Gate를 여러개 사용하여 XOR를 만들어낼 수 있듯이, Perceptron 역시 여러개의 Layer를 사용하면 XOR를 표현할 수 있었습니다.  \n",
    "\n",
    "그러면, 한번 XOR를 직접 학습해보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리들을 import 해보겠습니다.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋을 만들어볼게요.\n",
    "x_1, y_1 = np.random.multivariate_normal([2, 2], [[1, 0], [0, 1]], 500).T\n",
    "x_2, y_2 = np.random.multivariate_normal([-2, 2], [[1, 0], [0, 1]], 500).T\n",
    "x_3, y_3 = np.random.multivariate_normal([-2, -2], [[1, 0], [0, 1]], 500).T\n",
    "x_4, y_4 = np.random.multivariate_normal([2, -2], [[1, 0], [0, 1]], 500).T\n",
    "\n",
    "X = np.concatenate([x_1, x_3, x_2, x_4])\n",
    "Y = np.concatenate([y_1, y_3, y_2, y_4])\n",
    "Z = np.concatenate([np.ones(1000), np.zeros(1000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "# Move left y-axis and bottim x-axis to centre, passing through (0,0)\n",
    "ax.spines['left'].set_position('center')\n",
    "ax.spines['bottom'].set_position('center')\n",
    "\n",
    "# Eliminate upper and right axes\n",
    "ax.spines['right'].set_color('none')\n",
    "ax.spines['top'].set_color('none')\n",
    "\n",
    "# Show ticks in the left and lower axes only\n",
    "ax.xaxis.set_ticks_position('bottom')\n",
    "\n",
    "plt.scatter(X, Y, c=Z, cmap=plt.cm.Set3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1사분면과 3사분면엔 노란색의 데이터가 있고, 2사분면과 4사분면엔 민트색의 데이터가 있습니다!  \n",
    "자, 그러면 이 데이터를 가지고 지난번에 했던 Logistic Regression을 가지고 학습을 진행해볼게요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XORDataset(data.Dataset):\n",
    "    def __init__(self):\n",
    "        super(XORDataset, self).__init__()\n",
    "        \n",
    "        x_1, y_1 = np.random.multivariate_normal([2, 2], [[1, 0], [0, 1]], 500).T\n",
    "        x_2, y_2 = np.random.multivariate_normal([-2, 2], [[1, 0], [0, 1]], 500).T\n",
    "        x_3, y_3 = np.random.multivariate_normal([-2, -2], [[1, 0], [0, 1]], 500).T\n",
    "        x_4, y_4 = np.random.multivariate_normal([2, -2], [[1, 0], [0, 1]], 500).T\n",
    "\n",
    "        self.X = np.stack([np.concatenate([x_1, x_3, x_2, x_4]), np.concatenate([y_1, y_3, y_2, y_4])], axis=1)\n",
    "        self.Y = np.concatenate([np.ones(1000), np.zeros(1000)])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = torch.FloatTensor(self.X[index])\n",
    "        label = torch.FloatTensor([self.Y[index]])\n",
    "        return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearRegressionModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.sigmoid(self.fc1(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegressionModel().to(device)\n",
    "model(torch.FloatTensor([1, 2]).to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델을 가지고 학습 전, 현재 이 모델이 어떻게 분류를 하고 있는지 한번 볼게요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(X, Y, Z, model, save_idx=None, device='cuda'):\n",
    "    x_min, x_max = X.min() - .5, X.max() +.5\n",
    "    y_min, y_max = Y.min() - .5, Y.max() +.5\n",
    "\n",
    "    ar_X, ar_Y = np.meshgrid(np.arange(x_min, x_max, .02),\n",
    "                             np.arange(y_min, y_max, .02))\n",
    "\n",
    "    input_arange = np.array([ar_X.ravel(), ar_Y.ravel()])\n",
    "    input_arange = input_arange.T\n",
    "    Z_hat = model(torch.FloatTensor(input_arange).to(device)).detach().cpu().numpy()\n",
    "    Z_hat = Z_hat.round()\n",
    "    Z_hat = Z_hat.reshape(ar_X.shape)\n",
    "    \n",
    "    plt.figure(1)\n",
    "    plt.pcolormesh(ar_X, ar_Y, Z_hat, cmap=plt.cm.Set3)\n",
    "    plt.scatter(X, Y, c=Z, edgecolors='k', cmap=plt.cm.Set3)\n",
    "    plt.xlim(ar_X.min(), ar_X.max())\n",
    "    plt.ylim(ar_Y.min(), ar_Y.max())\n",
    "    \n",
    "    if save_idx != None:\n",
    "        if not os.path.exists(\"gifs\"):\n",
    "            os.mkdir(\"gifs\")\n",
    "        plt.savefig(\"gifs/\" + str(save_idx).zfill(5) + \".png\")\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = model.to(device)\n",
    "visualize(X, Y, Z, model, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "np.column_stack([X, Y, Z])자, 그럼 학습을 시작해볼게요.  \n",
    "지난번과 같이 Adam Optimizer를 통해 학습을 진행해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "dataset = XORDataset()\n",
    "dataloader = data.DataLoader(dataset, batch_size=512, shuffle=True)\n",
    "model = LinearRegressionModel().to(device)\n",
    "\n",
    "n_epoch = 200\n",
    "losses = []\n",
    "for epoch in range(n_epoch):\n",
    "    for x, y in dataloader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        # Initialize Optimizer\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Model Inference\n",
    "        y_hat = model(x)\n",
    "        \n",
    "        # Get Loss\n",
    "        loss = criterion(y_hat, y)\n",
    "        \n",
    "        # Logging\n",
    "        losses.append(loss.detach().cpu().item())\n",
    "        \n",
    "        # Back Propagation\n",
    "        loss.backward()\n",
    "        \n",
    "        # Weight Update\n",
    "        optimizer.step()\n",
    "    \n",
    "    if epoch%10 == 0:\n",
    "        print('EPOCH[{current}|{total}] loss: {loss:.2f}'.format(\n",
    "            current=epoch+1,\n",
    "            total=n_epoch,\n",
    "            loss=losses[-1]\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "자, 학습이 완료되었습니다!  \n",
    "학습이 되는동안 loss가 어떻게 떨어졌는지 Visualize해볼게요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.title(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss가 잘 안떨어지네요?   \n",
    "그러면 모델이 어떻게 학습했는지 Visualize를 해볼게요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(X, Y, Z, model, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "자, 그러면 Model을 바꿔서, Multi Layer Perceptron을 적용시켜봅시다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, 100)\n",
    "        self.fc2 = nn.Linear(100, 100)\n",
    "        self.fc3 = nn.Linear(100, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.sigmoid(self.fc1(x))\n",
    "        x = self.sigmoid(self.fc2(x))\n",
    "        x = self.sigmoid(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP().to(device)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "dataset = XORDataset()\n",
    "dataloader = data.DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "n_epoch = 200\n",
    "losses = []\n",
    "for epoch in range(n_epoch):\n",
    "    for x, y in dataloader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        # Initialize Optimizer\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Model Inference\n",
    "        y_hat = model(x)\n",
    "        \n",
    "        # Get Loss\n",
    "        loss = criterion(y_hat, y)\n",
    "        \n",
    "        # Logging\n",
    "        losses.append(loss.detach().cpu().item())\n",
    "        \n",
    "        # Back Propagation\n",
    "        loss.backward()\n",
    "        \n",
    "        # Weight Update\n",
    "        optimizer.step()\n",
    "    \n",
    "    if epoch%10 == 0:\n",
    "        print('EPOCH[{current}|{total}] loss: {loss:.2f}'.format(\n",
    "            current=epoch+1,\n",
    "            total=n_epoch,\n",
    "            loss=losses[-1]\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.title(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(X, Y, Z, model, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 수고하셨습니다"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
